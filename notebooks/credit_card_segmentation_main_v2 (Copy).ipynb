{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configure pandas options for better display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset from a given file path.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = load_data('../data/raw/Customer_Data.csv')\n",
    "\n",
    "# Standardize column names to lowercase\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# Drop customer ID as it's not needed for analysis\n",
    "if 'cust_id' in df.columns:\n",
    "    df.drop('cust_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"First 5 rows of the dataset:\\n\", df.head())\n",
    "print(\"Dataset description:\\n\", df.describe())\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "print(\"Random sample of data (transposed):\\n\", df.sample(5).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random samples from key columns with missing data\n",
    "print(\"Sample of minimum_payments:\\n\", df['minimum_payments'].sample(10))\n",
    "print(\"Sample of credit_limit:\\n\", df['credit_limit'].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using the median for columns with missing data\n",
    "columns_to_impute = ['minimum_payments', 'credit_limit']\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "print(\"Missing values after imputation:\\n\", df[columns_to_impute].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(df: pd.DataFrame, sample_limit: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects outliers in a DataFrame using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        sample_limit (int, optional): Maximum number of outliers to sample per column. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary DataFrame of detected outliers for each numerical column.\n",
    "    \"\"\"\n",
    "    outliers_summary = []\n",
    "    for column in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        Q1, Q3 = df[column].quantile(0.25), df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df)) * 100\n",
    "        # Collect statistics\n",
    "        summary = {\n",
    "            \"Column\": column,\n",
    "            \"Total Observations\": len(df),\n",
    "            \"Number of Outliers\": outlier_count,\n",
    "            \"Outlier Percentage (%)\": round(outlier_percentage, 2),\n",
    "            \"Mean\": round(df[column].mean(), 2),\n",
    "            \"Median\": round(df[column].median(), 2),\n",
    "            \"Standard Deviation\": round(df[column].std(), 2),\n",
    "            \"Skewness\": round(df[column].skew(), 2),\n",
    "            \"Kurtosis\": round(df[column].kurt(), 2),\n",
    "            \"Sample Outliers\": outliers.head(sample_limit).to_dict(orient='records') if outlier_count > 0 else \"None\"\n",
    "        }\n",
    "        outliers_summary.append(summary)\n",
    "    return pd.DataFrame(outliers_summary)\n",
    "\n",
    "# Detect outliers in the dataset\n",
    "outliers_summary_df = detect_outliers_iqr(df, sample_limit=3)\n",
    "\n",
    "# Display outlier summary\n",
    "print(\"Outlier Summary:\")\n",
    "print(outliers_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to treat outliers using capping method\n",
    "def treat_outliers(df: pd.DataFrame, outliers_summary_df: pd.DataFrame, method: str = 'cap') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Treat outliers by capping or removing them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        outliers_summary_df (pd.DataFrame): Outlier summary DataFrame.\n",
    "        method (str, optional): Method to treat outliers ('cap' or 'remove'). Defaults to 'cap'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with treated outliers.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    for index, row in outliers_summary_df.iterrows():\n",
    "        column = row['Column']\n",
    "        Q1, Q3 = df[column].quantile(0.25), df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        if method == 'cap':\n",
    "            # Cap outliers\n",
    "            df_cleaned[column] = df_cleaned[column].clip(lower_bound, upper_bound)\n",
    "        elif method == 'remove':\n",
    "            # Remove rows with outliers\n",
    "            outliers = df_cleaned[(df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)]\n",
    "            df_cleaned.drop(outliers.index, inplace=True)\n",
    "    return df_cleaned\n",
    "\n",
    "# Treat outliers using the capping method\n",
    "df_cleaned = treat_outliers(df, outliers_summary_df, method='cap')\n",
    "\n",
    "# Display cleaned data\n",
    "print(\"Cleaned Data (First 5 rows):\\n\", df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot box plots before and after outlier treatment\n",
    "def plot_boxplots_before_after(original_data: pd.DataFrame, cleaned_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot box plots to compare original and cleaned datasets before and after outlier removal.\n",
    "    \n",
    "    Args:\n",
    "        original_data (pd.DataFrame): The original DataFrame.\n",
    "        cleaned_data (pd.DataFrame): The cleaned DataFrame after outlier removal.\n",
    "    \"\"\"\n",
    "    # Get all numerical columns\n",
    "    numerical_columns = original_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Original data box plots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=original_data[numerical_columns])\n",
    "    plt.title('Box Plots Before Outlier Removal')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Cleaned data box plots\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(data=cleaned_data[numerical_columns])\n",
    "    plt.title('Box Plots After Outlier Removal')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot box plots for all features\n",
    "plot_boxplots_before_after(df, df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect skewness and kurtosis\n",
    "def detect_skewness_kurtosis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect skewness and kurtosis for numerical columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame summarizing skewness and kurtosis for each numerical column.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    for column in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        summary.append({\n",
    "            \"Column\": column,\n",
    "            \"Skewness\": round(df[column].skew(), 2),\n",
    "            \"Kurtosis\": round(df[column].kurt(), 2),\n",
    "            \"Mean\": round(df[column].mean(), 2),\n",
    "            \"Median\": round(df[column].median(), 2),\n",
    "            \"Standard Deviation\": round(df[column].std(), 2)\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Detect skewness and kurtosis\n",
    "skewness_kurtosis_summary_df = detect_skewness_kurtosis(df_cleaned)\n",
    "\n",
    "# Display skewness and kurtosis summary\n",
    "print(\"\\nSkewness and Kurtosis Summary:\")\n",
    "print(skewness_kurtosis_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix skewness and kurtosis\n",
    "def fix_skewness_kurtosis(df: pd.DataFrame, summary_df: pd.DataFrame, skew_threshold: float = 0.5, kurt_threshold: float = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix skewness and kurtosis in the dataset using transformations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        summary_df (pd.DataFrame): Summary DataFrame with skewness and kurtosis info.\n",
    "        skew_threshold (float, optional): Threshold to decide skewness. Defaults to 0.5.\n",
    "        kurt_threshold (float, optional): Threshold to decide kurtosis. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed DataFrame with fixed skewness and kurtosis.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    for _, row in summary_df.iterrows():\n",
    "        column = row['Column']\n",
    "        skewness, kurtosis = row['Skewness'], row['Kurtosis']\n",
    "        if skewness > skew_threshold:\n",
    "            # Apply log transformation for positive skewness\n",
    "            df_transformed[column] = np.log1p(df_transformed[column].clip(lower=0))\n",
    "        elif skewness < -skew_threshold:\n",
    "            # Apply square root transformation for negative skewness\n",
    "            df_transformed[column] = np.sqrt(df_transformed[column].clip(lower=0))\n",
    "        # Optionally handle kurtosis if needed (skipped here)\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix skewness in the dataset\n",
    "df_fixed_skewness = fix_skewness_kurtosis(df_cleaned, skewness_kurtosis_summary_df)\n",
    "\n",
    "# Display transformed data\n",
    "print(\"\\nTransformed Data (First 5 rows):\\n\", df_fixed_skewness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot skewness before and after fixing\n",
    "def plot_skewness_comparison(original_summary: pd.DataFrame, fixed_summary: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot comparison of skewness before and after fixing.\n",
    "\n",
    "    Args:\n",
    "        original_summary (pd.DataFrame): Summary before fixing.\n",
    "        fixed_summary (pd.DataFrame): Summary after fixing.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(original_summary))  # Get the range of indices for x-axis\n",
    "\n",
    "    # Plot the skewness before fixing\n",
    "    plt.bar(index, original_summary['skewness'], bar_width, label='Before Fixing', alpha=0.7)\n",
    "\n",
    "    # Plot the skewness after fixing\n",
    "    plt.bar(index + bar_width, fixed_summary['skewness'], bar_width, label='After Fixing', alpha=0.7)\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Skewness', fontsize=12)\n",
    "    plt.title('Comparison of Skewness Before and After Fixing', fontsize=15)\n",
    "    plt.xticks(index + bar_width / 2, original_summary.index, rotation=90)  # Feature names on x-axis\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  before and after summary\n",
    "original_summary = df.describe().T[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "fixed_summary = df_fixed_skewness.describe().T[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "\n",
    "# Adding skewness to summaries\n",
    "original_summary['skewness'] = df.skew()\n",
    "fixed_summary['skewness'] = df_fixed_skewness.skew()\n",
    "\n",
    "# Plot the comparison\n",
    "plot_skewness_comparison(original_summary, fixed_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional: Save the cleaned and transformed dataset to a new CSV file\n",
    "def save_cleaned_data(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the cleaned and transformed dataset to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        file_path (str): The path where the CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure that the directory exists\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Cleaned data saved to {file_path}\")\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "save_cleaned_data(df_fixed_skewness, '../data/processed/Cleaned_Customer_Data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution plot to visualize data distribution\n",
    "def plot_all_feature_distributions(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot histograms for the distributions of each numerical feature in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with cleaned data.\n",
    "    \"\"\"\n",
    "    # Plot histograms for all numerical columns in the DataFrame\n",
    "    hist_plot = df.hist(bins=50, figsize=(30, 20), grid=True, color='red', alpha=0.7)\n",
    "\n",
    "    # Set title for the overall plot\n",
    "    plt.suptitle('Distribution of Numerical Features', fontsize=20)\n",
    "    plt.xlabel('Value', fontsize=15)\n",
    "    plt.ylabel('Frequency', fontsize=15)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to prevent overlap with suptitle\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot distributions for cleaned data\n",
    "plot_all_feature_distributions(df_fixed_skewness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fixed_skewness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new variable for modeling\n",
    "df_model = df_fixed_skewness.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def summary_stats(df_model, n=4):\n",
    "    \"\"\"\n",
    "    Generate detailed descriptive statistics for the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df_model (pd.DataFrame): The DataFrame for which to calculate statistics.\n",
    "    n (int): Number of decimal places to round to.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing descriptive statistics for each attribute.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate central tendency\n",
    "    mean = pd.DataFrame(df_model.apply(np.mean)).T\n",
    "    median = pd.DataFrame(df_model.apply(np.median)).T\n",
    "    \n",
    "    # Calculate distribution statistics\n",
    "    std = pd.DataFrame(df_model.apply(np.std)).T\n",
    "    min_value = pd.DataFrame(df_model.apply(np.min)).T\n",
    "    max_value = pd.DataFrame(df_model.apply(np.max)).T\n",
    "    range_value = pd.DataFrame(df_model.apply(lambda x: x.max() - x.min())).T\n",
    "    skewness = pd.DataFrame(df_model.apply(lambda x: x.skew())).T\n",
    "    kurtosis = pd.DataFrame(df_model.apply(lambda x: x.kurtosis())).T\n",
    "    count = pd.DataFrame(df_model.count()).T  # Count of non-null values\n",
    "    \n",
    "    # Concatenate all statistics into a single DataFrame\n",
    "    summary_stats = pd.concat([count, min_value, max_value, range_value, mean, median, std, skewness, kurtosis]).T.reset_index()\n",
    "    summary_stats.columns = ['Attributes', 'Count', 'Min', 'Max', 'Range', 'Mean', 'Median', 'Std Dev', 'Skewness', 'Kurtosis']\n",
    "    \n",
    "    # Format the DataFrame for better readability\n",
    "    summary_stats['Min'] = summary_stats['Min'].round(n)\n",
    "    summary_stats['Max'] = summary_stats['Max'].round(n)\n",
    "    summary_stats['Range'] = summary_stats['Range'].round(n)\n",
    "    summary_stats['Mean'] = summary_stats['Mean'].round(n)\n",
    "    summary_stats['Median'] = summary_stats['Median'].round(n)\n",
    "    summary_stats['Std Dev'] = summary_stats['Std Dev'].round(n)\n",
    "    summary_stats['Skewness'] = summary_stats['Skewness'].round(n)\n",
    "    summary_stats['Kurtosis'] = summary_stats['Kurtosis'].round(n)\n",
    "\n",
    "    # Add a summary section title\n",
    "    print(\"Summary Statistics:\")\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "# descriptive analytic for modeling feature\n",
    "summary_stats(df_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Correlation Matrix and Visualization\n",
    "correlations = df_model.corr()\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(correlations, annot=True, cmap=\"Blues\")\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame containing the relevant features\n",
    "correlations = df_model.corr()\n",
    "\n",
    "# 1. Set the diagonal to NaN to avoid considering self-correlation\n",
    "correlation_matrix_no_diag = correlations.where(~np.eye(correlations.shape[0], dtype=bool))\n",
    "\n",
    "# 2. Find the maximum correlation value and its corresponding feature pair\n",
    "max_corr_value = correlation_matrix_no_diag.max().max()  # Find the maximum correlation value\n",
    "max_corr_indices = correlation_matrix_no_diag.stack().idxmax()  # Find the corresponding feature pair\n",
    "\n",
    "# 3. Find the minimum correlation value and its corresponding feature pair\n",
    "min_corr_value = correlation_matrix_no_diag.min().min()  # Find the minimum correlation value\n",
    "min_corr_indices = correlation_matrix_no_diag.stack().idxmin()  # Find the corresponding feature pair\n",
    "\n",
    "# 4. Display the results\n",
    "print(f\"Maximum correlation value: {max_corr_value} between features: {max_corr_indices}\")\n",
    "print(f\"Minimum correlation value: {min_corr_value} between features: {min_corr_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame containing the relevant features\n",
    "correlations = df_model.corr()\n",
    "\n",
    "# 1. Set the diagonal to NaN to avoid considering self-correlation\n",
    "correlation_matrix_no_diag = correlations.where(~np.eye(correlations.shape[0], dtype=bool))\n",
    "\n",
    "# Define correlation thresholds\n",
    "positive_threshold = 0.7  # Adjust this based on your needs\n",
    "negative_threshold = 0.5  # Adjust this based on your needs\n",
    "\n",
    "# 2. Find positive correlations exceeding the threshold\n",
    "positive_corr_pairs = correlation_matrix_no_diag.stack()[\n",
    "    correlation_matrix_no_diag.stack() > positive_threshold\n",
    "].reset_index()\n",
    "\n",
    "positive_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "print(\"Positive correlation pairs (correlation > 0.5):\")\n",
    "print(positive_corr_pairs)\n",
    "\n",
    "# 3. Find negative correlations below the threshold\n",
    "negative_corr_pairs = correlation_matrix_no_diag.stack()[\n",
    "    correlation_matrix_no_diag.stack() < negative_threshold\n",
    "].reset_index()\n",
    "\n",
    "negative_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "print(\"\\nNegative correlation pairs (correlation < -0.5):\")\n",
    "print(negative_corr_pairs)\n",
    "\n",
    "# 4. Display the maximum and minimum correlation values and their corresponding feature pairs again for context\n",
    "max_corr_value = correlation_matrix_no_diag.max().max()\n",
    "max_corr_indices = correlation_matrix_no_diag.stack().idxmax()\n",
    "\n",
    "min_corr_value = correlation_matrix_no_diag.min().min()\n",
    "min_corr_indices = correlation_matrix_no_diag.stack().idxmin()\n",
    "\n",
    "print(f\"\\nMaximum correlation value: {max_corr_value} between features: {max_corr_indices}\")\n",
    "print(f\"Minimum correlation value: {min_corr_value} between features: {min_corr_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [\n",
    "    'purchases_frequency',\n",
    "    'oneoff_purchases_frequency',\n",
    "    'purchases_trx',\n",
    "    'cash_advance_trx',\n",
    "    'purchases_installments_frequency'\n",
    "]\n",
    "\n",
    "# Dropping features from df_model_features\n",
    "df_model_features = df_model.drop(columns=features_to_drop)\n",
    "\n",
    "df_model_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_features.dtypes, df_model_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Create New Features\n",
    "df_model_features['purchase_to_payment_ratio'] = df_model_features['purchases'] / (df_model_features['payments'] + 1e-5)\n",
    "df_model_features['credit_utilization'] = df_model_features['balance'] / (df_model_features['credit_limit'] + 1e-5)\n",
    "df_model_features['avg_purchases_per_month'] = df_model_features['purchases'] / df_model_features['tenure']\n",
    "df_model_features['avg_payments_per_month'] = df_model_features['payments'] / df_model_features['tenure']\n",
    "df_model_features['purchase_payment_interaction'] = df_model_features['purchases'] * df_model_features['payments']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df_model_features is your DataFrame\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_model_features)\n",
    "\n",
    "# Create a DataFrame from the scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=df_model_features.columns)\n",
    "\n",
    "# Display the first few rows of the scaled DataFrame\n",
    "scaled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Set style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your scaled DataFrame\n",
    "# Example: scaled_df = pd.read_csv('path_to_your_scaled_data.csv')\n",
    "\n",
    "# Display basic information about the data\n",
    "print(scaled_df.info())\n",
    "print(scaled_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(scaled_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for each feature\n",
    "scaled_df.hist(figsize=(15, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a pair plot for the scaled DataFrame\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.pairplot(scaled_df)\n",
    "# plt.suptitle('Pair Plot of Scaled DataFrame', y=1.02)  # Adjust title position\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method to find optimal number of clusters\n",
    "inertia = []\n",
    "range_n_clusters = range(1, 11)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(scaled_df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with the chosen number of clusters (e.g., from elbow method)\n",
    "optimal_clusters_kmeans = 4  # Adjust based on elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters_kmeans, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(scaled_df)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "scaled_df['KMeans_Cluster'] = kmeans_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linkage methods\n",
    "linkage_methods = ['ward', 'average', 'complete']\n",
    "agglomerative_results = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    # Fit Agglomerative Clustering with the optimal number of clusters\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=optimal_clusters_kmeans, linkage=method)\n",
    "    agglomerative_labels = agglomerative.fit_predict(scaled_df)\n",
    "\n",
    "    # Store results\n",
    "    agglomerative_results[method] = agglomerative_labels\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    scaled_df[f'Agglomerative_Cluster_{method}'] = agglomerative_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram for Ward method\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram = sch.dendrogram(sch.linkage(scaled_df, method='ward'))\n",
    "plt.title('Dendrogram for Ward Linkage')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(scaled_df.drop(columns=['KMeans_Cluster'] + [f'Agglomerative_Cluster_{m}' for m in linkage_methods]))\n",
    "\n",
    "# Create a new DataFrame for visualization\n",
    "tsne_df = pd.DataFrame(data=tsne_result, columns=['TSNE1', 'TSNE2'])\n",
    "tsne_df['KMeans_Cluster'] = kmeans_labels\n",
    "\n",
    "# Plotting t-SNE results for K-Means\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='KMeans_Cluster', data=tsne_df, palette='deep', legend='full', alpha=0.7)\n",
    "plt.title('t-SNE Visualization of K-Means Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette scores for K-Means\n",
    "kmeans_silhouette = silhouette_score(scaled_df, kmeans_labels)\n",
    "print(f'K-Means Silhouette Score: {kmeans_silhouette:.4f}')\n",
    "\n",
    "# Calculate silhouette scores for Agglomerative Clustering\n",
    "for method in linkage_methods:\n",
    "    silhouette = silhouette_score(scaled_df, agglomerative_results[method])\n",
    "    print(f'Agglomerative Clustering ({method}) Silhouette Score: {silhouette:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Davies-Bouldin scores for K-Means\n",
    "kmeans_davies_bouldin = davies_bouldin_score(scaled_df, kmeans_labels)\n",
    "print(f'K-Means Davies-Bouldin Score: {kmeans_davies_bouldin:.4f}')\n",
    "\n",
    "# Calculate Davies-Bouldin scores for Agglomerative Clustering\n",
    "for method in linkage_methods:\n",
    "    davies_bouldin = davies_bouldin_score(scaled_df, agglomerative_results[method])\n",
    "    print(f'Agglomerative Clustering ({method}) Davies-Bouldin Score: {davies_bouldin:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Davies-Bouldin scores\n",
    "kmeans_davies_bouldin = davies_bouldin_score(scaled_df, kmeans_labels)\n",
    "agglomerative_davies_bouldin = davies_bouldin_score(scaled_df, agglomerative_labels)\n",
    "\n",
    "print(f'K-Means Davies-Bouldin Score: {kmeans_davies_bouldin:.4f}')\n",
    "print(f'Agglomerative Davies-Bouldin Score: {agglomerative_davies_bouldin:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.columns, scaled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assume k_optimal is the chosen number of clusters (you may have determined this using the elbow method)\n",
    "k_optimal = 4\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "clusters_kmeans = kmeans.fit_predict(scaled_df)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "scaled_df['Cluster'] = clusters_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the clusters based on the mean of each feature\n",
    "cluster_profile = scaled_df.groupby('Cluster').mean()\n",
    "print(cluster_profile)\n",
    "\n",
    "# Optional: View other statistics such as median or count\n",
    "cluster_profile_median = scaled_df.groupby('Cluster').median()\n",
    "cluster_profile_count = scaled_df.groupby('Cluster').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transpose the profile data for easier plotting\n",
    "cluster_profile_T = cluster_profile.T\n",
    "\n",
    "# List of columns to exclude\n",
    "columns_to_exclude = [\n",
    "    'KMeans_Cluster',\n",
    "    'Agglomerative_Cluster_ward',\n",
    "    'Agglomerative_Cluster_average',\n",
    "    'Agglomerative_Cluster_complete'\n",
    "]\n",
    "\n",
    "# Print the initial DataFrame\n",
    "print(\"Initial DataFrame:\\n\", cluster_profile_T.head())\n",
    "print(\"Initial columns:\", cluster_profile_T.columns)\n",
    "\n",
    "# Exclude specified columns\n",
    "cluster_profile_T = cluster_profile_T.drop(columns=columns_to_exclude, errors='ignore')\n",
    "\n",
    "# Print shape after exclusion\n",
    "print(\"Shape after exclusion:\", cluster_profile_T.shape)\n",
    "\n",
    "# Plotting\n",
    "cluster_profile_T.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Cluster Profiling - Feature Means')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed profile with mean, median, min, max for key features in KMeans\n",
    "detailed_kmeans_profile = scaled_df.groupby('KMeans_Cluster').agg({\n",
    "    'balance': ['mean', 'median', 'min', 'max'],\n",
    "    'balance_frequency': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'purchases': ['mean', 'median', 'min', 'max'],\n",
    "    'oneoff_purchases': ['mean', 'median', 'min', 'max'],\n",
    "    'installments_purchases': ['mean', 'median', 'min', 'max'],\n",
    "    'cash_advance': ['mean', 'median', 'min', 'max'],\n",
    "    'cash_advance_frequency': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'credit_limit': ['mean', 'median', 'min', 'max'],\n",
    "    'payments': ['mean', 'median', 'min', 'max'],\n",
    "    'minimum_payments': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'prc_full_payment': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'tenure': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'purchase_to_payment_ratio': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'credit_utilization': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'avg_purchases_per_month': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'avg_payments_per_month': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "    'purchase_payment_interaction': ['mean', 'median', 'min', 'max'],  # Added feature\n",
    "})\n",
    "\n",
    "# View the detailed KMeans cluster profile\n",
    "print(\"\\nDetailed KMeans Cluster Profile:\\n\", detailed_kmeans_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Profile Summary\n",
    "\n",
    "## Overall Summary\n",
    "The clustering analysis identified four distinct customer segments based on credit card usage, revealing insights for marketing strategies and risk management.\n",
    "\n",
    "## Cluster Profiles\n",
    "\n",
    "### Cluster 0: High Balance and Moderate Purchases\n",
    "- **Balance**: \n",
    "  - Mean: 0.74 (Effective management)\n",
    "- **Purchases**: \n",
    "  - Mean: 0.53 (Regular utilization)\n",
    "- **Payments**: \n",
    "  - Mean: 0.42 (Reliable)\n",
    "- **Credit Limit**: \n",
    "  - Mean: 0.26 (Stable)\n",
    "- **Cash Advance**: \n",
    "  - Mean: 1.07 (Liquidity needs)\n",
    "\n",
    "**Interpretation**: Financially stable customers with active usage and reliable payments.\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster 1: Low Balance with High Purchases\n",
    "- **Balance**: \n",
    "  - Mean: -1.47 (Poor management)\n",
    "- **Purchases**: \n",
    "  - Mean: 0.08 (Limited usage)\n",
    "- **Payments**: \n",
    "  - Mean: -0.98 (Financial stress)\n",
    "- **Credit Limit**: \n",
    "  - Mean: -0.34 (Need for borrowing)\n",
    "- **Cash Advance**: \n",
    "  - Mean: -0.71 (Minimal reliance)\n",
    "\n",
    "**Interpretation**: Active spenders facing financial challenges.\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster 2: Moderate Balance with Variable Purchases\n",
    "- **Balance**: \n",
    "  - Mean: 0.11 (Inconsistent management)\n",
    "- **Purchases**: \n",
    "  - Mean: 0.69 (Reliance on credit)\n",
    "- **Payments**: \n",
    "  - Mean: 0.32 (Moderate reliability)\n",
    "- **Credit Limit**: \n",
    "  - Mean: 0.13 (Typical behavior)\n",
    "- **Cash Advance**: \n",
    "  - Mean: -0.85 (Limited needs)\n",
    "\n",
    "**Interpretation**: Mix of financial stability and variable spending habits.\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster 3: Balanced Usage with Inconsistent Payments\n",
    "- **Balance**: \n",
    "  - Mean: 0.56 (Stable management)\n",
    "- **Purchases**: \n",
    "  - Mean: -1.65 (Limited usage)\n",
    "- **Payments**: \n",
    "  - Mean: 0.04 (Possible difficulties)\n",
    "- **Credit Limit**: \n",
    "  - Mean: -0.12 (Average access)\n",
    "- **Cash Advance**: \n",
    "  - Mean: 0.99 (Occasional needs)\n",
    "\n",
    "**Interpretation**: Balanced spending with financial instability affecting payments.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "These profiles facilitate tailored marketing strategies and improved risk management, enhancing customer satisfaction and retention.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bia_capstone_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
